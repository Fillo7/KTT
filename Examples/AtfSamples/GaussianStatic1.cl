#define TYPE_T float
#define TYPE_TS float

// helper macros
#if   OCL_DIM_L_1 == 1
  #define CONCAT_IN_DESCENDING_OCL_ORDER_1(i, j) i

  #define CONCAT_IN_DESCENDING_OCL_ORDER_0(i, j) j
#elif OCL_DIM_L_2 == 1
  #define CONCAT_IN_DESCENDING_OCL_ORDER_1(i, j) j

  #define CONCAT_IN_DESCENDING_OCL_ORDER_0(i, j) i
#endif
#define CONCAT_IN_DESCENDING_OCL_ORDER(i, j) CONCAT_IN_DESCENDING_OCL_ORDER_1(i, j)CONCAT_IN_DESCENDING_OCL_ORDER_0(i, j)

#if   OCL_DIM_L_1 == 1
  #define FLAT_INDEX_IN_DESCENDING_OCL_ORDER_1(i_id, j_id, i_size, j_size) i_id

  #define FLAT_INDEX_IN_DESCENDING_OCL_ORDER_0(i_id, j_id, i_size, j_size) (FLAT_INDEX_IN_DESCENDING_OCL_ORDER_1(i_id, j_id, i_size, j_size)) * (j_size) + (j_id)
#elif OCL_DIM_L_2 == 1
  #define FLAT_INDEX_IN_DESCENDING_OCL_ORDER_1(i_id, j_id, i_size, j_size) j_id

  #define FLAT_INDEX_IN_DESCENDING_OCL_ORDER_0(i_id, j_id, i_size, j_size) (FLAT_INDEX_IN_DESCENDING_OCL_ORDER_1(i_id, j_id, i_size, j_size)) * (i_size) + (i_id)
#endif
#define FLAT_INDEX_IN_DESCENDING_OCL_ORDER(i_id, j_id, i_size, j_size) FLAT_INDEX_IN_DESCENDING_OCL_ORDER_0(i_id, j_id, i_size, j_size)

#if   OCL_DIM_L_1 == 1
  #define DESCENDING_L_DIMS_1(i, j) i

  #define DESCENDING_L_DIMS_0(i, j) j
#elif OCL_DIM_L_2 == 1
  #define DESCENDING_L_DIMS_1(i, j) j

  #define DESCENDING_L_DIMS_0(i, j) i
#endif

#define GET_GLOBAL_ID_L_1   get_global_id(OCL_DIM_L_1)
#define GET_LOCAL_ID_L_1    get_local_id(OCL_DIM_L_1)
#define GET_GROUP_ID_L_1    get_group_id(OCL_DIM_L_1)
#define GET_GLOBAL_SIZE_L_1 (NUM_WG_L_1 * NUM_WI_L_1)
#define GET_LOCAL_SIZE_L_1  NUM_WI_L_1
#define GET_GLOBAL_ID_L_2   get_global_id(OCL_DIM_L_2)
#define GET_LOCAL_ID_L_2    get_local_id(OCL_DIM_L_2)
#define GET_GROUP_ID_L_2    get_group_id(OCL_DIM_L_2)
#define GET_GLOBAL_SIZE_L_2 (NUM_WG_L_2 * NUM_WI_L_2)
#define GET_LOCAL_SIZE_L_2  NUM_WI_L_2

#define PRIVATE 0
#define LOCAL   1
#define GLOBAL  2

// =============== macro definitions per dimension ============================
// -------------------- L_1 --------------------

// functional unit ids
#define K1_G_FU_ID_L_1 i_wg_l_1
#define K1_L_FU_ID_L_1 i_wi_l_1
#define K1_P_FU_ID_L_1 0

// number of functional units
#define K1_G_NUM_FU_L_1 NUM_WG_L_1
#define K1_L_NUM_FU_L_1 NUM_WI_L_1
#define K1_P_NUM_FU_L_1 1

// cache block sizes
#define K1_G_CB_SIZE_L_1 INPUT_SIZE_L_1
#define K1_L_CB_SIZE_L_1 L_CB_SIZE_L_1
#define K1_P_CB_SIZE_L_1 P_CB_SIZE_L_1

// number of steps per FU
#define K1_G_NUM_STEPS_L_1 1
#define K1_L_NUM_STEPS_L_1 (K1_G_NUM_CACHED_ITERATIONS_L_1 / K1_L_NUM_CACHED_ITERATIONS_L_1)
#define K1_P_NUM_STEPS_L_1 (K1_L_NUM_CACHED_ITERATIONS_L_1 / K1_P_NUM_CACHED_ITERATIONS_L_1)

// number of cached iterations per FU
#define K1_G_NUM_CACHED_ITERATIONS_L_1 (K1_G_CB_SIZE_L_1 / GET_GLOBAL_SIZE_L_1)
#define K1_L_NUM_CACHED_ITERATIONS_L_1 (K1_L_CB_SIZE_L_1 / GET_LOCAL_SIZE_L_1)
#define K1_P_NUM_CACHED_ITERATIONS_L_1 (K1_P_CB_SIZE_L_1)

// number of extra cached iterations per FU
#define K1_G_NUM_EXTRA_CACHED_ITERATIONS_L_1 0
#define K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_1 (K1_G_CB_SIZE_L_1 % (GET_GLOBAL_SIZE_L_1 * K1_L_NUM_CACHED_ITERATIONS_L_1) / GET_GLOBAL_SIZE_L_1)
#define K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 ((GET_LOCAL_SIZE_L_1 * K1_L_NUM_CACHED_ITERATIONS_L_1) % (GET_LOCAL_SIZE_L_1 * K1_P_NUM_CACHED_ITERATIONS_L_1) / GET_LOCAL_SIZE_L_1)

// number of extra elements
#define K1_G_NUM_EXTRA_ELEMENTS_L_1 0
#define K1_L_NUM_EXTRA_ELEMENTS_L_1 (K1_G_CB_SIZE_L_1 % (GET_GLOBAL_SIZE_L_1 * K1_L_NUM_CACHED_ITERATIONS_L_1) % GET_GLOBAL_SIZE_L_1)
#define K1_P_NUM_EXTRA_ELEMENTS_L_1 ((GET_LOCAL_SIZE_L_1 * K1_L_NUM_CACHED_ITERATIONS_L_1) % (GET_LOCAL_SIZE_L_1 * K1_P_NUM_CACHED_ITERATIONS_L_1) % GET_LOCAL_SIZE_L_1)

// number of processed elements
#define K1_G_NUM_PROCESSED_ELEMENTS_L_1 ((K1_L_NUM_STEPS_L_1 * K1_L_NUM_CACHED_ITERATIONS_L_1 + K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_1) * GET_GLOBAL_SIZE_L_1 + K1_L_NUM_EXTRA_ELEMENTS_L_1)
#define K1_L_NUM_PROCESSED_ELEMENTS_L_1 ((K1_P_NUM_STEPS_L_1 * K1_P_NUM_CACHED_ITERATIONS_L_1 + K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1) * GET_LOCAL_SIZE_L_1 + K1_P_NUM_EXTRA_ELEMENTS_L_1)
#define K1_P_NUM_PROCESSED_ELEMENTS_L_1 K1_P_NUM_CACHED_ITERATIONS_L_1

// incomplete cache block sizes
#define K1_P_INCOMPLETE_CB_SIZE_IN_COMPLETE_L_CB_L_1 (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1)
#define K1_P_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_1 (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1)
#define K1_L_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_1 (K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_1 * GET_LOCAL_SIZE_L_1)

// number of steps per FU in incomplete cache blocks
#define K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 (K1_L_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 / K1_P_NUM_CACHED_ITERATIONS_L_1)

// number of cached iterations per FU in incomplete cache blocks
#define K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1 (K1_P_INCOMPLETE_CB_SIZE_IN_COMPLETE_L_CB_L_1)
#define K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 (K1_P_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_1)
#define K1_L_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 (K1_L_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_1 / GET_LOCAL_SIZE_L_1)

// number of extra cached iterations per FU in incomplete cache blocks
#if K1_P_NUM_CACHED_ITERATIONS_L_1 > 0
#define K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 ((GET_LOCAL_SIZE_L_1 * K1_L_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1) % (GET_LOCAL_SIZE_L_1 * K1_P_NUM_CACHED_ITERATIONS_L_1) / GET_LOCAL_SIZE_L_1)
#else
#define K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 0
#endif

// number of extra elements in incomplete cache blocks
#if K1_P_NUM_CACHED_ITERATIONS_L_1 > 0
#define K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 ((GET_LOCAL_SIZE_L_1 * K1_L_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1) % (GET_LOCAL_SIZE_L_1 * K1_P_NUM_CACHED_ITERATIONS_L_1) % GET_LOCAL_SIZE_L_1)
#else
#define K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 0
#endif

// number of processed elements in incomplete cache blocks
#define K1_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1
#define K1_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1
#define K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 ((K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 * K1_P_NUM_CACHED_ITERATIONS_L_1 + K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1) * GET_LOCAL_SIZE_L_1 + K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1)

// index conversion
#define K1_L_TO_G_INDEX_L_1(i) ((l_step_l_1 * K1_L_NUM_CACHED_ITERATIONS_L_1 + (i) / K1_L_NUM_FU_L_1) * GET_GLOBAL_SIZE_L_1 + K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1 + ((i) % K1_L_NUM_FU_L_1))
#define K1_P_TO_L_INDEX_L_1(i) ((p_step_l_1 * K1_P_NUM_CACHED_ITERATIONS_L_1 + (i) / K1_P_NUM_FU_L_1) * GET_LOCAL_SIZE_L_1 + K1_L_FU_ID_L_1 * K1_P_NUM_FU_L_1 + ((i) % K1_P_NUM_FU_L_1))

// index conversion in phase 3
#define K1_P3_L_TO_G_INDEX_L_1(i) (K1_G_CB_SIZE_L_1 / GET_GLOBAL_SIZE_L_1 * GET_GLOBAL_SIZE_L_1 + K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1 + (i))
#define K1_P3_P_TO_L_INDEX_L_1(i) (K1_L_CB_SIZE_L_1 / GET_LOCAL_SIZE_L_1 * GET_LOCAL_SIZE_L_1 + K1_L_FU_ID_L_1 * K1_P_NUM_FU_L_1 + (i))
#define K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i) (K1_L_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_1 / GET_LOCAL_SIZE_L_1 * GET_LOCAL_SIZE_L_1 + K1_L_FU_ID_L_1 * K1_P_NUM_FU_L_1 + (i))
#define K1_PP3_P_TO_L_INDEX_L_1(i) (K1_L_FU_ID_L_1 * K1_P_NUM_FU_L_1 + (i))

// iteration to index conversion
#define K1_L_ITERATION_TO_L_INDEX_L_1(i) ((i) * K1_L_NUM_FU_L_1 + K1_L_FU_ID_L_1)
#define K1_P_ITERATION_TO_P_INDEX_L_1(i) ((i) * K1_P_NUM_FU_L_1 + K1_P_FU_ID_L_1)


// -------------------- L_2 --------------------

// functional unit ids
#define K1_G_FU_ID_L_2 i_wg_l_2
#define K1_L_FU_ID_L_2 i_wi_l_2
#define K1_P_FU_ID_L_2 0

// number of functional units
#define K1_G_NUM_FU_L_2 NUM_WG_L_2
#define K1_L_NUM_FU_L_2 NUM_WI_L_2
#define K1_P_NUM_FU_L_2 1

// cache block sizes
#define K1_G_CB_SIZE_L_2 INPUT_SIZE_L_2
#define K1_L_CB_SIZE_L_2 L_CB_SIZE_L_2
#define K1_P_CB_SIZE_L_2 P_CB_SIZE_L_2

// number of steps per FU
#define K1_G_NUM_STEPS_L_2 1
#define K1_L_NUM_STEPS_L_2 (K1_G_NUM_CACHED_ITERATIONS_L_2 / K1_L_NUM_CACHED_ITERATIONS_L_2)
#define K1_P_NUM_STEPS_L_2 (K1_L_NUM_CACHED_ITERATIONS_L_2 / K1_P_NUM_CACHED_ITERATIONS_L_2)

// number of cached iterations per FU
#define K1_G_NUM_CACHED_ITERATIONS_L_2 (K1_G_CB_SIZE_L_2 / GET_GLOBAL_SIZE_L_2)
#define K1_L_NUM_CACHED_ITERATIONS_L_2 (K1_L_CB_SIZE_L_2 / GET_LOCAL_SIZE_L_2)
#define K1_P_NUM_CACHED_ITERATIONS_L_2 (K1_P_CB_SIZE_L_2)

// number of extra cached iterations per FU
#define K1_G_NUM_EXTRA_CACHED_ITERATIONS_L_2 0
#define K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2 (K1_G_CB_SIZE_L_2 % (GET_GLOBAL_SIZE_L_2 * K1_L_NUM_CACHED_ITERATIONS_L_2) / GET_GLOBAL_SIZE_L_2)
#define K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 ((GET_LOCAL_SIZE_L_2 * K1_L_NUM_CACHED_ITERATIONS_L_2) % (GET_LOCAL_SIZE_L_2 * K1_P_NUM_CACHED_ITERATIONS_L_2) / GET_LOCAL_SIZE_L_2)

// number of extra elements
#define K1_G_NUM_EXTRA_ELEMENTS_L_2 0
#define K1_L_NUM_EXTRA_ELEMENTS_L_2 (K1_G_CB_SIZE_L_2 % (GET_GLOBAL_SIZE_L_2 * K1_L_NUM_CACHED_ITERATIONS_L_2) % GET_GLOBAL_SIZE_L_2)
#define K1_P_NUM_EXTRA_ELEMENTS_L_2 ((GET_LOCAL_SIZE_L_2 * K1_L_NUM_CACHED_ITERATIONS_L_2) % (GET_LOCAL_SIZE_L_2 * K1_P_NUM_CACHED_ITERATIONS_L_2) % GET_LOCAL_SIZE_L_2)

// number of processed elements
#define K1_G_NUM_PROCESSED_ELEMENTS_L_2 ((K1_L_NUM_STEPS_L_2 * K1_L_NUM_CACHED_ITERATIONS_L_2 + K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2) * GET_GLOBAL_SIZE_L_2 + K1_L_NUM_EXTRA_ELEMENTS_L_2)
#define K1_L_NUM_PROCESSED_ELEMENTS_L_2 ((K1_P_NUM_STEPS_L_2 * K1_P_NUM_CACHED_ITERATIONS_L_2 + K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2) * GET_LOCAL_SIZE_L_2 + K1_P_NUM_EXTRA_ELEMENTS_L_2)
#define K1_P_NUM_PROCESSED_ELEMENTS_L_2 K1_P_NUM_CACHED_ITERATIONS_L_2

// incomplete cache block sizes
#define K1_P_INCOMPLETE_CB_SIZE_IN_COMPLETE_L_CB_L_2 (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2)
#define K1_P_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_2 (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2)
#define K1_L_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_2 (K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2 * GET_LOCAL_SIZE_L_2)

// number of steps per FU in incomplete cache blocks
#define K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 (K1_L_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 / K1_P_NUM_CACHED_ITERATIONS_L_2)

// number of cached iterations per FU in incomplete cache blocks
#define K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2 (K1_P_INCOMPLETE_CB_SIZE_IN_COMPLETE_L_CB_L_2)
#define K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 (K1_P_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_2)
#define K1_L_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 (K1_L_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_2 / GET_LOCAL_SIZE_L_2)

// number of extra cached iterations per FU in incomplete cache blocks
#if K1_P_NUM_CACHED_ITERATIONS_L_2 > 0
#define K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 ((GET_LOCAL_SIZE_L_2 * K1_L_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2) % (GET_LOCAL_SIZE_L_2 * K1_P_NUM_CACHED_ITERATIONS_L_2) / GET_LOCAL_SIZE_L_2)
#else
#define K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 0
#endif

// number of extra elements in incomplete cache blocks
#if K1_P_NUM_CACHED_ITERATIONS_L_2 > 0
#define K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 ((GET_LOCAL_SIZE_L_2 * K1_L_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2) % (GET_LOCAL_SIZE_L_2 * K1_P_NUM_CACHED_ITERATIONS_L_2) % GET_LOCAL_SIZE_L_2)
#else
#define K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 0
#endif

// number of processed elements in incomplete cache blocks
#define K1_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2 K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2
#define K1_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2 K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2
#define K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2 ((K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 * K1_P_NUM_CACHED_ITERATIONS_L_2 + K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2) * GET_LOCAL_SIZE_L_2 + K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2)

// index conversion
#define K1_L_TO_G_INDEX_L_2(i) ((l_step_l_2 * K1_L_NUM_CACHED_ITERATIONS_L_2 + (i) / K1_L_NUM_FU_L_2) * GET_GLOBAL_SIZE_L_2 + K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2 + ((i) % K1_L_NUM_FU_L_2))
#define K1_P_TO_L_INDEX_L_2(i) ((p_step_l_2 * K1_P_NUM_CACHED_ITERATIONS_L_2 + (i) / K1_P_NUM_FU_L_2) * GET_LOCAL_SIZE_L_2 + K1_L_FU_ID_L_2 * K1_P_NUM_FU_L_2 + ((i) % K1_P_NUM_FU_L_2))

// index conversion in phase 3
#define K1_P3_L_TO_G_INDEX_L_2(i) (K1_G_CB_SIZE_L_2 / GET_GLOBAL_SIZE_L_2 * GET_GLOBAL_SIZE_L_2 + K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2 + (i))
#define K1_P3_P_TO_L_INDEX_L_2(i) (K1_L_CB_SIZE_L_2 / GET_LOCAL_SIZE_L_2 * GET_LOCAL_SIZE_L_2 + K1_L_FU_ID_L_2 * K1_P_NUM_FU_L_2 + (i))
#define K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(i) (K1_L_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_2 / GET_LOCAL_SIZE_L_2 * GET_LOCAL_SIZE_L_2 + K1_L_FU_ID_L_2 * K1_P_NUM_FU_L_2 + (i))
#define K1_PP3_P_TO_L_INDEX_L_2(i) (K1_L_FU_ID_L_2 * K1_P_NUM_FU_L_2 + (i))

// iteration to index conversion
#define K1_L_ITERATION_TO_L_INDEX_L_2(i) ((i) * K1_L_NUM_FU_L_2 + K1_L_FU_ID_L_2)
#define K1_P_ITERATION_TO_P_INDEX_L_2(i) ((i) * K1_P_NUM_FU_L_2 + K1_P_FU_ID_L_2)


// -------------------- combined over dimensions --------------------

// flat WI ids
#define K1_G_FLAT_WI_ID (FLAT_INDEX_IN_DESCENDING_OCL_ORDER(GET_GLOBAL_ID_L_1, GET_GLOBAL_ID_L_2, GET_GLOBAL_SIZE_L_1, GET_GLOBAL_SIZE_L_2))
#define K1_L_FLAT_WI_ID (FLAT_INDEX_IN_DESCENDING_OCL_ORDER(GET_LOCAL_ID_L_1, GET_LOCAL_ID_L_2, GET_LOCAL_SIZE_L_1, GET_LOCAL_SIZE_L_2))
#define K1_P_FLAT_WI_ID (0)

// flat number of WIs
#define K1_G_FLAT_NUM_WI (K1_G_NUM_FU_L_1 * K1_L_NUM_FU_L_1 * K1_G_NUM_FU_L_2 * K1_L_NUM_FU_L_2)
#define K1_L_FLAT_NUM_WI (K1_L_NUM_FU_L_1 * K1_L_NUM_FU_L_2)
#define K1_P_FLAT_NUM_WI (1)
// =============== end of macro definitions per dimension =====================

// =============== macro definitions per buffer ===============================
// -------------------- buffer IN --------------------

// index conversion
#define K1_IN_L_TO_G_INDEX_L_1(i) ((l_step_l_1 * K1_L_NUM_CACHED_ITERATIONS_L_1 + (i) / (2 + K1_L_NUM_FU_L_1 + 2)) * (0 + GET_GLOBAL_SIZE_L_1 + 0) + K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1 + ((i) % (2 + K1_L_NUM_FU_L_1 + 2)))
#define K1_IN_L_TO_G_INDEX_L_2(i) ((l_step_l_2 * K1_L_NUM_CACHED_ITERATIONS_L_2 + (i) / (2 + K1_L_NUM_FU_L_2 + 2)) * (0 + GET_GLOBAL_SIZE_L_2 + 0) + K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2 + ((i) % (2 + K1_L_NUM_FU_L_2 + 2)))
#define K1_IN_P_TO_L_INDEX_L_1(i) ((p_step_l_1 * K1_P_NUM_CACHED_ITERATIONS_L_1 + (i) / (2 + K1_P_NUM_FU_L_1 + 2)) * (2 + GET_LOCAL_SIZE_L_1 + 2) + K1_L_FU_ID_L_1 * K1_P_NUM_FU_L_1 + ((i) % (2 + K1_P_NUM_FU_L_1 + 2)))
#define K1_IN_P_TO_L_INDEX_L_2(i) ((p_step_l_2 * K1_P_NUM_CACHED_ITERATIONS_L_2 + (i) / (2 + K1_P_NUM_FU_L_2 + 2)) * (2 + GET_LOCAL_SIZE_L_2 + 2) + K1_L_FU_ID_L_2 * K1_P_NUM_FU_L_2 + ((i) % (2 + K1_P_NUM_FU_L_2 + 2)))

// index conversion in phase 3
#define K1_IN_P3_L_TO_G_INDEX_L_1(i) (K1_G_CB_SIZE_L_1 / GET_GLOBAL_SIZE_L_1 * GET_GLOBAL_SIZE_L_1 + K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1 + (i))
#define K1_IN_P3_L_TO_G_INDEX_L_2(i) (K1_G_CB_SIZE_L_2 / GET_GLOBAL_SIZE_L_2 * GET_GLOBAL_SIZE_L_2 + K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2 + (i))
#define K1_IN_P3_P_TO_L_INDEX_L_1(i) (K1_L_CB_SIZE_L_1 / GET_LOCAL_SIZE_L_1 * GET_LOCAL_SIZE_L_1 + K1_L_FU_ID_L_1 * K1_P_NUM_FU_L_1 + (i))
#define K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i) (K1_L_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_1 / GET_LOCAL_SIZE_L_1 * GET_LOCAL_SIZE_L_1 + K1_L_FU_ID_L_1 * K1_P_NUM_FU_L_1 + (i))
#define K1_IN_PP3_P_TO_L_INDEX_L_1(i) (K1_L_FU_ID_L_1 * K1_P_NUM_FU_L_1 + (i))
#define K1_IN_P3_P_TO_L_INDEX_L_2(i) (K1_L_CB_SIZE_L_2 / GET_LOCAL_SIZE_L_2 * GET_LOCAL_SIZE_L_2 + K1_L_FU_ID_L_2 * K1_P_NUM_FU_L_2 + (i))
#define K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(i) (K1_L_INCOMPLETE_CB_SIZE_IN_COMPLETE_G_CB_L_2 / GET_LOCAL_SIZE_L_2 * GET_LOCAL_SIZE_L_2 + K1_L_FU_ID_L_2 * K1_P_NUM_FU_L_2 + (i))
#define K1_IN_PP3_P_TO_L_INDEX_L_2(i) (K1_L_FU_ID_L_2 * K1_P_NUM_FU_L_2 + (i))

// iteration to index conversion
#define K1_IN_L_ITERATION_TO_L_INDEX_L_1(i) (2 + (i) * (2 + K1_L_NUM_FU_L_1 + 2) + K1_L_FU_ID_L_1)
#define K1_IN_L_ITERATION_TO_L_INDEX_L_2(i) (2 + (i) * (2 + K1_L_NUM_FU_L_2 + 2) + K1_L_FU_ID_L_2)
#define K1_IN_P_ITERATION_TO_P_INDEX_L_1(i) (2 + (i) * (2 + K1_P_NUM_FU_L_1 + 2) + K1_P_FU_ID_L_1)
#define K1_IN_P_ITERATION_TO_P_INDEX_L_2(i) (2 + (i) * (2 + K1_P_NUM_FU_L_2 + 2) + K1_P_FU_ID_L_2)

// buffer abstraction
#if   OCL_DIM_L_1 == 1
  #define BUFFER_IN_INDEX_1(i, j) i
  #define BUFFER_IN_G_SIZE_1 (2 + K1_G_CB_SIZE_L_1 + 2)
  #define BUFFER_IN_L_SIZE_1 (K1_L_NUM_CACHED_ITERATIONS_L_1 * (2 + K1_L_NUM_FU_L_1 + 2) + ((K1_L_CB_SIZE_L_1 % K1_L_NUM_FU_L_1) > 0) * (2 + (K1_L_CB_SIZE_L_1 % K1_L_NUM_FU_L_1) + 2))
  #define BUFFER_IN_P_SIZE_1 (K1_P_NUM_CACHED_ITERATIONS_L_1 * (2 + K1_P_NUM_FU_L_1 + 2) + ((K1_P_CB_SIZE_L_1 % K1_P_NUM_FU_L_1) > 0) * (2 + (K1_P_CB_SIZE_L_1 % K1_P_NUM_FU_L_1) + 2))

  #define BUFFER_IN_INDEX_0(i, j) j
  #define BUFFER_IN_G_SIZE_0 (2 + K1_G_CB_SIZE_L_2 + 2)
  #define BUFFER_IN_L_SIZE_0 (K1_L_NUM_CACHED_ITERATIONS_L_2 * (2 + K1_L_NUM_FU_L_2 + 2) + ((K1_L_CB_SIZE_L_2 % K1_L_NUM_FU_L_2) > 0) * (2 + (K1_L_CB_SIZE_L_2 % K1_L_NUM_FU_L_2) + 2))
  #define BUFFER_IN_P_SIZE_0 (K1_P_NUM_CACHED_ITERATIONS_L_2 * (2 + K1_P_NUM_FU_L_2 + 2) + ((K1_P_CB_SIZE_L_2 % K1_P_NUM_FU_L_2) > 0) * (2 + (K1_P_CB_SIZE_L_2 % K1_P_NUM_FU_L_2) + 2))
#elif OCL_DIM_L_2 == 1
  #define BUFFER_IN_INDEX_1(i, j) j
  #define BUFFER_IN_G_SIZE_1 (2 + K1_G_CB_SIZE_L_2 + 2)
  #define BUFFER_IN_L_SIZE_1 (K1_L_NUM_CACHED_ITERATIONS_L_2 * (2 + K1_L_NUM_FU_L_2 + 2) + ((K1_L_CB_SIZE_L_2 % K1_L_NUM_FU_L_2) > 0) * (2 + (K1_L_CB_SIZE_L_2 % K1_L_NUM_FU_L_2) + 2))
  #define BUFFER_IN_P_SIZE_1 (K1_P_NUM_CACHED_ITERATIONS_L_2 * (2 + K1_P_NUM_FU_L_2 + 2) + ((K1_P_CB_SIZE_L_2 % K1_P_NUM_FU_L_2) > 0) * (2 + (K1_P_CB_SIZE_L_2 % K1_P_NUM_FU_L_2) + 2))

  #define BUFFER_IN_INDEX_0(i, j) i
  #define BUFFER_IN_G_SIZE_0 (2 + K1_G_CB_SIZE_L_1 + 2)
  #define BUFFER_IN_L_SIZE_0 (K1_L_NUM_CACHED_ITERATIONS_L_1 * (2 + K1_L_NUM_FU_L_1 + 2) + ((K1_L_CB_SIZE_L_1 % K1_L_NUM_FU_L_1) > 0) * (2 + (K1_L_CB_SIZE_L_1 % K1_L_NUM_FU_L_1) + 2))
  #define BUFFER_IN_P_SIZE_0 (K1_P_NUM_CACHED_ITERATIONS_L_1 * (2 + K1_P_NUM_FU_L_1 + 2) + ((K1_P_CB_SIZE_L_1 % K1_P_NUM_FU_L_1) > 0) * (2 + (K1_P_CB_SIZE_L_1 % K1_P_NUM_FU_L_1) + 2))
#endif
#define K1_G_BUFFER_IN(i, j) in[((i)) * (2 + K1_G_CB_SIZE_L_2 + 2) + ((j))]
#define K1_L_BUFFER_IN(i, j) cb_l_in[(BUFFER_IN_INDEX_1(i, j))][(BUFFER_IN_INDEX_0(i, j))]
#define K1_P_BUFFER_IN(i, j) cb_p_in[(BUFFER_IN_INDEX_1(i, j))][(BUFFER_IN_INDEX_0(i, j))]

// partitioning and cache usage
#define K1_IN_G_NUM_PROCESSED_ELEMENTS_L_1 ((K1_L_NUM_STEPS_L_1 * K1_L_NUM_CACHED_ITERATIONS_L_1 + K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_1) * (2 + GET_GLOBAL_SIZE_L_1 + 2) + (K1_L_NUM_EXTRA_ELEMENTS_L_1 > 0) * (2 + K1_L_NUM_EXTRA_ELEMENTS_L_1 + 2))
#define K1_IN_G_NUM_PROCESSED_ELEMENTS_L_2 ((K1_L_NUM_STEPS_L_2 * K1_L_NUM_CACHED_ITERATIONS_L_2 + K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2) * (2 + GET_GLOBAL_SIZE_L_2 + 2) + (K1_L_NUM_EXTRA_ELEMENTS_L_2 > 0) * (2 + K1_L_NUM_EXTRA_ELEMENTS_L_2 + 2))
#define K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 ((K1_P_NUM_STEPS_L_1 * K1_P_NUM_CACHED_ITERATIONS_L_1 + K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1) * (2 + GET_LOCAL_SIZE_L_1 + 2) + (K1_P_NUM_EXTRA_ELEMENTS_L_1 > 0) * (2 + K1_P_NUM_EXTRA_ELEMENTS_L_1 + 2))
#define K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 ((K1_P_NUM_STEPS_L_2 * K1_P_NUM_CACHED_ITERATIONS_L_2 + K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2) * (2 + GET_LOCAL_SIZE_L_2 + 2) + (K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0) * (2 + K1_P_NUM_EXTRA_ELEMENTS_L_2 + 2))
#define K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 (K1_P_NUM_CACHED_ITERATIONS_L_1 * (2 + K1_P_NUM_FU_L_1 + 2))
#define K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 (K1_P_NUM_CACHED_ITERATIONS_L_2 * (2 + K1_P_NUM_FU_L_2 + 2))

#define K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 (K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1 * (2 + K1_P_NUM_FU_L_1 + 2))
#define K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2 (K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2 * (2 + K1_P_NUM_FU_L_2 + 2))
#define K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 (K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 * (2 + K1_P_NUM_FU_L_1 + 2))
#define K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2 (K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 * (2 + K1_P_NUM_FU_L_2 + 2))
#define K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 ((K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 * K1_P_NUM_CACHED_ITERATIONS_L_1 + K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1) * (2 + GET_LOCAL_SIZE_L_1 + 2) + (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 > 0) * (2 + K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 + 2))
#define K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2 ((K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 * K1_P_NUM_CACHED_ITERATIONS_L_2 + K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2) * (2 + GET_LOCAL_SIZE_L_2 + 2) + (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0) * (2 + K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + 2))

#define K1_G_MEM_IN(i, j) K1_G_BUFFER_IN(i, j)
#if CACHE_L_CB != 0
#define K1_L_MEM_IN(i, j) K1_L_BUFFER_IN(i, j)
#define K1_P3_L_2_L_MEM_IN(i, j) K1_L_BUFFER_IN(i, j)
#define K1_P3_L_1_L_MEM_IN(i, j) K1_L_BUFFER_IN(i, j)
#define K1_P3_L_1_P3_L_2_L_MEM_IN(i, j) K1_L_BUFFER_IN(i, j)
#else
#define K1_L_MEM_IN(i, j) K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(i), K1_IN_L_TO_G_INDEX_L_2(j))
#define K1_P3_L_2_L_MEM_IN(i, j) K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(i), K1_IN_P3_L_TO_G_INDEX_L_2(j))
#define K1_P3_L_1_L_MEM_IN(i, j) K1_G_MEM_IN(K1_IN_P3_L_TO_G_INDEX_L_1(i), K1_IN_L_TO_G_INDEX_L_2(j))
#define K1_P3_L_1_P3_L_2_L_MEM_IN(i, j) K1_G_MEM_IN(K1_IN_P3_L_TO_G_INDEX_L_1(i), K1_IN_P3_L_TO_G_INDEX_L_2(j))
#endif
#if CACHE_P_CB != 0
#define K1_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_P3_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_PP3_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_P3_L_1_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_P3_L_1_P3_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_P3_L_1_PP3_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_PP3_L_1_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_PP3_L_1_P3_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#define K1_PP3_L_1_PP3_L_2_P_MEM_IN(i, j) K1_P_BUFFER_IN(i, j)
#else
#define K1_P_MEM_IN(i, j) K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(i), K1_IN_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_2_P_MEM_IN(i, j) K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(i), K1_IN_P3_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(i, j) K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(i), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_PP3_L_2_P_MEM_IN(i, j) K1_P3_L_2_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(i), K1_IN_PP3_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_1_P_MEM_IN(i, j) K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(i), K1_IN_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_1_P3_L_2_P_MEM_IN(i, j) K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(i), K1_IN_P3_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(i, j) K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(i), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_P3_L_1_PP3_L_2_P_MEM_IN(i, j) K1_P3_L_2_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(i), K1_IN_PP3_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(i, j) K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_IN_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(i, j) K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_IN_P3_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(i, j) K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(i, j) K1_P3_L_2_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_IN_PP3_P_TO_L_INDEX_L_2(j))
#define K1_PP3_L_1_P_MEM_IN(i, j) K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(i), K1_IN_P_TO_L_INDEX_L_2(j))
#define K1_PP3_L_1_P3_L_2_P_MEM_IN(i, j) K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(i), K1_IN_P3_P_TO_L_INDEX_L_2(j))
#define K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(i, j) K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(i), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_PP3_L_1_PP3_L_2_P_MEM_IN(i, j) K1_P3_L_1_P3_L_2_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(i), K1_IN_PP3_P_TO_L_INDEX_L_2(j))
#endif



// -------------------- result buffer --------------------

// check which levels are used
#if G_CB_RES_DEST_LEVEL == PRIVATE || L_CB_RES_DEST_LEVEL == PRIVATE || P_CB_RES_DEST_LEVEL == PRIVATE
#define K1_P_LEVEL_HAS_RESULTS
#endif
#if G_CB_RES_DEST_LEVEL == LOCAL || L_CB_RES_DEST_LEVEL == LOCAL || P_CB_RES_DEST_LEVEL == LOCAL
#define K1_L_LEVEL_HAS_RESULTS
#endif
#if G_CB_RES_DEST_LEVEL == GLOBAL || L_CB_RES_DEST_LEVEL == GLOBAL || P_CB_RES_DEST_LEVEL == GLOBAL
#define K1_G_LEVEL_HAS_RESULTS
#endif

// ------ PRIVATE ------
#ifdef K1_P_LEVEL_HAS_RESULTS
// construct prefix for res_p
#if G_CB_RES_DEST_LEVEL == PRIVATE
#define K1_RES_P_BUFFER_G_PREFIX_L_1() [l_step_l_1]
#define K1_RES_P_BUFFER_G_PREFIX_L_2() [l_step_l_2]
#define K1_RES_P_BUFFER_DEF_G_PREFIX_L_1() [K1_L_NUM_STEPS_L_1 + (K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0) + (K1_L_NUM_EXTRA_ELEMENTS_L_1 > 0)]
#define K1_RES_P_BUFFER_DEF_G_PREFIX_L_2() [K1_L_NUM_STEPS_L_2 + (K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0) + (K1_L_NUM_EXTRA_ELEMENTS_L_2 > 0)]
#else
#define K1_RES_P_BUFFER_G_PREFIX_L_1()
#define K1_RES_P_BUFFER_G_PREFIX_L_2()
#define K1_RES_P_BUFFER_DEF_G_PREFIX_L_1()
#define K1_RES_P_BUFFER_DEF_G_PREFIX_L_2()
#endif
#if L_CB_RES_DEST_LEVEL == PRIVATE
#define K1_RES_P_BUFFER_L_PREFIX_L_1() [p_step_l_1]
#define K1_RES_P_BUFFER_L_PREFIX_L_2() [p_step_l_2]
#define K1_RES_P_BUFFER_DEF_L_PREFIX_L_1() [K1_P_NUM_STEPS_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0) + (K1_P_NUM_EXTRA_ELEMENTS_L_1 > 0)]
#define K1_RES_P_BUFFER_DEF_L_PREFIX_L_2() [K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0) + (K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0)]
#else
#define K1_RES_P_BUFFER_L_PREFIX_L_1()
#define K1_RES_P_BUFFER_L_PREFIX_L_2()
#define K1_RES_P_BUFFER_DEF_L_PREFIX_L_1()
#define K1_RES_P_BUFFER_DEF_L_PREFIX_L_2()
#endif
// buffer abstraction for res_p
#define K1_RES_P_BUFFER_NAME() res_p
#define K1_RES_P_BUFFER_DEF K1_RES_P_BUFFER_NAME()CONCAT_IN_DESCENDING_OCL_ORDER(K1_RES_P_BUFFER_DEF_G_PREFIX_L_1()K1_RES_P_BUFFER_DEF_L_PREFIX_L_1()[K1_P_CB_SIZE_L_1], K1_RES_P_BUFFER_DEF_G_PREFIX_L_2()K1_RES_P_BUFFER_DEF_L_PREFIX_L_2()[K1_P_CB_SIZE_L_2])
#define K1_RES_P_BUFFER(i, j) K1_RES_P_BUFFER_NAME()CONCAT_IN_DESCENDING_OCL_ORDER(K1_RES_P_BUFFER_G_PREFIX_L_1()K1_RES_P_BUFFER_L_PREFIX_L_1()[i], K1_RES_P_BUFFER_G_PREFIX_L_2()K1_RES_P_BUFFER_L_PREFIX_L_2()[j])
#endif

// ------ LOCAL ------
#ifdef K1_L_LEVEL_HAS_RESULTS
// construct prefix for res_l
#if G_CB_RES_DEST_LEVEL == LOCAL
#define K1_RES_L_BUFFER_G_PREFIX_L_1() [l_step_l_1]
#define K1_RES_L_BUFFER_G_PREFIX_L_2() [l_step_l_2]
#define K1_RES_L_BUFFER_DEF_G_PREFIX_L_1() [K1_L_NUM_STEPS_L_1 + (K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0) + (K1_L_NUM_EXTRA_ELEMENTS_L_1 > 0)]
#define K1_RES_L_BUFFER_DEF_G_PREFIX_L_2() [K1_L_NUM_STEPS_L_2 + (K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0) + (K1_L_NUM_EXTRA_ELEMENTS_L_2 > 0)]
#else
#define K1_RES_L_BUFFER_G_PREFIX_L_1()
#define K1_RES_L_BUFFER_G_PREFIX_L_2()
#define K1_RES_L_BUFFER_DEF_G_PREFIX_L_1()
#define K1_RES_L_BUFFER_DEF_G_PREFIX_L_2()
#endif
// buffer abstraction for res_l
#define K1_RES_L_BUFFER_NAME() res_l
#define K1_RES_L_BUFFER_DEF K1_RES_L_BUFFER_NAME()CONCAT_IN_DESCENDING_OCL_ORDER(K1_RES_L_BUFFER_DEF_G_PREFIX_L_1()[K1_L_CB_SIZE_L_1], K1_RES_L_BUFFER_DEF_G_PREFIX_L_2()[K1_L_CB_SIZE_L_2])
#define K1_RES_L_BUFFER(i, j) K1_RES_L_BUFFER_NAME()CONCAT_IN_DESCENDING_OCL_ORDER(K1_RES_L_BUFFER_G_PREFIX_L_1()[i], K1_RES_L_BUFFER_G_PREFIX_L_2()[j])
#endif

// ------ GLOBAL ------
#ifdef K1_G_LEVEL_HAS_RESULTS
// buffer abstraction for res_g
#define K1_RES_G_BUFFER_NAME() int_res
#define K1_RES_G_BUFFER(i, j) K1_RES_G_BUFFER_NAME()[(i) * K1_G_CB_SIZE_L_2 + (j)]
#endif

// determine memory destination for results
#if   P_CB_RES_DEST_LEVEL == PRIVATE
#define K1_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_L_1_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_L_1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_L_1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_IN_COMPLETE_G_CB_L_1_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_PP3_L_1_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_PP3_L_1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_PP3_L_1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#elif P_CB_RES_DEST_LEVEL == LOCAL
#define K1_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P_TO_L_INDEX_L_1(i), K1_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P_TO_L_INDEX_L_1(i), K1_PP3_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_1_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P3_P_TO_L_INDEX_L_1(i), K1_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P3_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P3_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_P3_L_1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P3_P_TO_L_INDEX_L_1(i), K1_PP3_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_P3_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_PP3_P_TO_L_INDEX_L_2(j))
#define K1_PP3_L_1_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_PP3_P_TO_L_INDEX_L_1(i), K1_P_TO_L_INDEX_L_2(j))
#define K1_PP3_L_1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_PP3_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_L_2(j))
#define K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_PP3_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_PP3_L_1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_PP3_P_TO_L_INDEX_L_1(i), K1_PP3_P_TO_L_INDEX_L_2(j))
#elif P_CB_RES_DEST_LEVEL == GLOBAL
#define K1_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P_TO_L_INDEX_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P_TO_L_INDEX_L_2(j)))
#define K1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P_TO_L_INDEX_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P3_P_TO_L_INDEX_L_2(j)))
#define K1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P_TO_L_INDEX_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j)))
#define K1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P_TO_L_INDEX_L_1(i)), K1_P3_L_TO_G_INDEX_L_2(K1_PP3_P_TO_L_INDEX_L_2(j)))
#define K1_P3_L_1_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P3_P_TO_L_INDEX_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P_TO_L_INDEX_L_2(j)))
#define K1_P3_L_1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P3_P_TO_L_INDEX_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P3_P_TO_L_INDEX_L_2(j)))
#define K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P3_P_TO_L_INDEX_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j)))
#define K1_P3_L_1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P3_P_TO_L_INDEX_L_1(i)), K1_P3_L_TO_G_INDEX_L_2(K1_PP3_P_TO_L_INDEX_L_2(j)))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P_TO_L_INDEX_L_2(j)))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P3_P_TO_L_INDEX_L_2(j)))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j)))
#define K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i)), K1_P3_L_TO_G_INDEX_L_2(K1_PP3_P_TO_L_INDEX_L_2(j)))
#define K1_PP3_L_1_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(K1_PP3_P_TO_L_INDEX_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P_TO_L_INDEX_L_2(j)))
#define K1_PP3_L_1_P3_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(K1_PP3_P_TO_L_INDEX_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P3_P_TO_L_INDEX_L_2(j)))
#define K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(K1_PP3_P_TO_L_INDEX_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j)))
#define K1_PP3_L_1_PP3_L_2_P_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(K1_PP3_P_TO_L_INDEX_L_1(i)), K1_P3_L_TO_G_INDEX_L_2(K1_PP3_P_TO_L_INDEX_L_2(j)))
#endif

#if   L_CB_RES_DEST_LEVEL == PRIVATE
#define K1_L_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_L_2_L_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_L_1_L_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#define K1_P3_L_1_P3_L_2_L_CB_RES_DEST(i, j) K1_RES_P_BUFFER(i, j)
#elif L_CB_RES_DEST_LEVEL == LOCAL
#define K1_L_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P_TO_L_INDEX_L_1(i), K1_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_2_L_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_P_TO_L_INDEX_L_1(i), K1_PP3_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_1_L_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_PP3_P_TO_L_INDEX_L_1(i), K1_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_1_P3_L_2_L_CB_RES_DEST(i, j) K1_RES_L_BUFFER(K1_PP3_P_TO_L_INDEX_L_1(i), K1_PP3_P_TO_L_INDEX_L_2(j))
#elif L_CB_RES_DEST_LEVEL == GLOBAL
#define K1_L_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P_TO_L_INDEX_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P_TO_L_INDEX_L_2(j)))
#define K1_P3_L_2_L_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(K1_P_TO_L_INDEX_L_1(i)), K1_P3_L_TO_G_INDEX_L_2(K1_PP3_P_TO_L_INDEX_L_2(j)))
#define K1_P3_L_1_L_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(K1_PP3_P_TO_L_INDEX_L_1(i)), K1_L_TO_G_INDEX_L_2(K1_P_TO_L_INDEX_L_2(j)))
#define K1_P3_L_1_P3_L_2_L_CB_RES_DEST(i, j) K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(K1_PP3_P_TO_L_INDEX_L_1(i)), K1_P3_L_TO_G_INDEX_L_2(K1_PP3_P_TO_L_INDEX_L_2(j)))
#endif


// buffer abstraction for kernel_res buffer
#define K1_KERNEL_RES_BUFFER(i, j) int_res[(i) * K1_G_CB_SIZE_L_2 + (j)]

#define K1_G_KERNEL_RES(i, j) K1_KERNEL_RES_BUFFER(i, j)
#define K1_L_KERNEL_RES(i, j) K1_G_KERNEL_RES(K1_L_TO_G_INDEX_L_1(i), K1_L_TO_G_INDEX_L_2(j))
#define K1_P3_L_2_L_KERNEL_RES(i, j) K1_G_KERNEL_RES(K1_L_TO_G_INDEX_L_1(i), K1_P3_L_TO_G_INDEX_L_2(j))
#define K1_P3_L_1_L_KERNEL_RES(i, j) K1_G_KERNEL_RES(K1_P3_L_TO_G_INDEX_L_1(i), K1_L_TO_G_INDEX_L_2(j))
#define K1_P3_L_1_P3_L_2_L_KERNEL_RES(i, j) K1_G_KERNEL_RES(K1_P3_L_TO_G_INDEX_L_1(i), K1_P3_L_TO_G_INDEX_L_2(j))
#define K1_P_KERNEL_RES(i, j) K1_L_KERNEL_RES(K1_P_TO_L_INDEX_L_1(i), K1_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_2_P_KERNEL_RES(i, j) K1_L_KERNEL_RES(K1_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_2_P_KERNEL_RES(i, j) K1_L_KERNEL_RES(K1_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_PP3_L_2_P_KERNEL_RES(i, j) K1_P3_L_2_L_KERNEL_RES(K1_P_TO_L_INDEX_L_1(i), K1_PP3_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_1_P_KERNEL_RES(i, j) K1_L_KERNEL_RES(K1_P3_P_TO_L_INDEX_L_1(i), K1_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_1_P3_L_2_P_KERNEL_RES(i, j) K1_L_KERNEL_RES(K1_P3_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_L_2(j))
#define K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_KERNEL_RES(i, j) K1_L_KERNEL_RES(K1_P3_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_P3_L_1_PP3_L_2_P_KERNEL_RES(i, j) K1_P3_L_2_L_KERNEL_RES(K1_P3_P_TO_L_INDEX_L_1(i), K1_PP3_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P_KERNEL_RES(i, j) K1_L_KERNEL_RES(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_KERNEL_RES(i, j) K1_L_KERNEL_RES(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_P3_P_TO_L_INDEX_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_KERNEL_RES(i, j) K1_L_KERNEL_RES(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_KERNEL_RES(i, j) K1_P3_L_2_L_KERNEL_RES(K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(i), K1_PP3_P_TO_L_INDEX_L_2(j))
#define K1_PP3_L_1_P_KERNEL_RES(i, j) K1_P3_L_1_L_KERNEL_RES(K1_PP3_P_TO_L_INDEX_L_1(i), K1_P_TO_L_INDEX_L_2(j))
#define K1_PP3_L_1_P3_L_2_P_KERNEL_RES(i, j) K1_P3_L_1_L_KERNEL_RES(K1_PP3_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_L_2(j))
#define K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_KERNEL_RES(i, j) K1_P3_L_1_L_KERNEL_RES(K1_PP3_P_TO_L_INDEX_L_1(i), K1_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(j))
#define K1_PP3_L_1_PP3_L_2_P_KERNEL_RES(i, j) K1_P3_L_1_P3_L_2_L_KERNEL_RES(K1_PP3_P_TO_L_INDEX_L_1(i), K1_PP3_P_TO_L_INDEX_L_2(j))
// =============== end of macro definitions per buffer ========================

// =============== scalar function ============================================
inline TYPE_TS f(const TYPE_T in_val_l1_m2_l2_m2, const TYPE_T in_val_l1_m2_l2_m1, const TYPE_T in_val_l1_m2, const TYPE_T in_val_l1_m2_l2_p1, const TYPE_T in_val_l1_m2_l2_p2, const TYPE_T in_val_l1_m1_l2_m2, const TYPE_T in_val_l1_m1_l2_m1, const TYPE_T in_val_l1_m1, const TYPE_T in_val_l1_m1_l2_p1, const TYPE_T in_val_l1_m1_l2_p2, const TYPE_T in_val_l2_m2, const TYPE_T in_val_l2_m1, const TYPE_T in_val, const TYPE_T in_val_l2_p1, const TYPE_T in_val_l2_p2, const TYPE_T in_val_l1_p1_l2_m2, const TYPE_T in_val_l1_p1_l2_m1, const TYPE_T in_val_l1_p1, const TYPE_T in_val_l1_p1_l2_p1, const TYPE_T in_val_l1_p1_l2_p2, const TYPE_T in_val_l1_p2_l2_m2, const TYPE_T in_val_l1_p2_l2_m1, const TYPE_T in_val_l1_p2, const TYPE_T in_val_l1_p2_l2_p1, const TYPE_T in_val_l1_p2_l2_p2) {
  
  return (2.0f * in_val_l1_m2_l2_m2 +  4.0f * in_val_l1_m2_l2_m1 +  5.0f * in_val_l1_m2 +  4.0f * in_val_l1_m2_l2_p1 + 2.0f * in_val_l1_m2_l2_p2 +
          4.0f * in_val_l1_m1_l2_m2 +  9.0f * in_val_l1_m1_l2_m1 + 12.0f * in_val_l1_m1 +  9.0f * in_val_l1_m1_l2_p1 + 4.0f * in_val_l1_m1_l2_p2 +
          5.0f * in_val_l2_m2       + 12.0f * in_val_l2_m1       + 15.0f * in_val       + 12.0f * in_val_l2_p1       + 5.0f * in_val_l2_p2       +
          4.0f * in_val_l1_p1_l2_m2 +  9.0f * in_val_l1_p1_l2_m1 + 12.0f * in_val_l1_p1 +  9.0f * in_val_l1_p1_l2_p1 + 4.0f * in_val_l1_p1_l2_p2 +
          2.0f * in_val_l1_p2_l2_m2 +  4.0f * in_val_l1_p2_l2_m1 +  5.0f * in_val_l1_p2 +  4.0f * in_val_l1_p2_l2_p1 + 2.0f * in_val_l1_p2_l2_p2);
}
// =============== end of scalar function =====================================

// =============== kernel 1 ===================================================
__kernel void gaussian_1(__global TYPE_T const * const restrict in, __global TYPE_TS * const restrict res_g, __global TYPE_TS * const restrict int_res) {
  // map md_hom dimensions to OpenCL dimensions
  const size_t i_wg_l_1 = GET_GROUP_ID_L_1;
  const size_t i_wi_l_1 = GET_LOCAL_ID_L_1;
  
  const size_t i_wg_l_2 = GET_GROUP_ID_L_2;
  const size_t i_wi_l_2 = GET_LOCAL_ID_L_2;
  
  // declare variables for caching inputs
  #if CACHE_L_CB != 0
  __local TYPE_T cb_l_in[BUFFER_IN_L_SIZE_1][BUFFER_IN_L_SIZE_0];
  #endif
  #if CACHE_P_CB != 0
  __private TYPE_T cb_p_in[BUFFER_IN_P_SIZE_1][BUFFER_IN_P_SIZE_0];
  #endif

  // declare variables for result memory
  // ------ LOCAL ------
  #ifdef K1_L_LEVEL_HAS_RESULTS
  __local TYPE_TS K1_RES_L_BUFFER_DEF;
  #endif
  // ------ PRIVATE ------
  #ifdef K1_P_LEVEL_HAS_RESULTS
  __private TYPE_TS K1_RES_P_BUFFER_DEF;
  #endif

  // phase 1: process local cached iterations in dimension L_1 with all FUs
  #if K1_L_NUM_STEPS_L_1 > 0
  for (size_t l_step_l_1 = 0; l_step_l_1 < K1_L_NUM_STEPS_L_1; ++l_step_l_1) {
    // phase 1: process local cached iterations in dimension L_2 with all FUs
    #if K1_L_NUM_STEPS_L_2 > 0
    for (size_t l_step_l_2 = 0; l_step_l_2 < K1_L_NUM_STEPS_L_2; ++l_step_l_2) {
      // ---------- L caching --------------------
      #if CACHE_L_CB != 0
      #if (K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_L_FLAT_NUM_WI > 0
      for (size_t step = 0; step < (K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2;
        K1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      #endif
      #if (K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) % K1_L_FLAT_NUM_WI > 0
      if (K1_L_FLAT_WI_ID < (K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2;
        K1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      #endif
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      // ---------- end of L caching -------------
      
      
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_L_1; ++p_step_l_1) {
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
        #endif
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
        #endif
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0);
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_L_1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
          #endif
        
        }
        #endif
      }
      #endif
      
      
      // wait for all WIs to finish computation on local cache block
      #if CACHE_L_CB != 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      
      // move results upwards in memory hierarchy
      #if G_CB_RES_DEST_LEVEL > L_CB_RES_DEST_LEVEL
      
      #if L_CB_RES_DEST_LEVEL < LOCAL
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_L_1; ++p_step_l_1) {
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
        #endif
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
        #endif
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0);
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P3_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_P3_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_1_P3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
        }
        #endif
      }
      #endif
      #else
      #if L_CB_RES_DEST_LEVEL <= LOCAL && CACHE_L_CB == 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #if (K1_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_L_FLAT_NUM_WI > 0
      for (size_t step = 0; step < (K1_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, K1_L_NUM_PROCESSED_ELEMENTS_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        #endif
        K1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #endif
      #if (K1_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_L_2) % K1_L_FLAT_NUM_WI > 0
      if (K1_L_FLAT_WI_ID < (K1_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_L_2) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + ((K1_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, K1_L_NUM_PROCESSED_ELEMENTS_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        #endif
        K1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #endif
      #if L_CB_RES_DEST_LEVEL <= LOCAL
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #endif
      #endif
    
    } // end of "l_step_l_2"-loop
    #endif
    // phase 2: process extra local iterations in dimension L_2 with all FUs
    #if K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
    {  
      const size_t l_step_l_2 = K1_L_NUM_STEPS_L_2;
      // ---------- L caching --------------------
      #if CACHE_L_CB != 0
      #if ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI > 0
      for (size_t step = 0; step < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        K1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      #endif
      #if ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_L_FLAT_NUM_WI > 0
      if (K1_L_FLAT_WI_ID < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + (((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        K1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      #endif
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      // ---------- end of L caching -------------
      
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_L_1; ++p_step_l_1) {
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
        #endif
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
        #endif
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0);
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
          #endif
        
        }
        #endif
      }
      #endif
      
      // wait for all WIs to finish computation on local cache block
      #if CACHE_L_CB != 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      
      // move results upwards in memory hierarchy
      #if G_CB_RES_DEST_LEVEL > L_CB_RES_DEST_LEVEL
      
      #if L_CB_RES_DEST_LEVEL < LOCAL
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_L_1; ++p_step_l_1) {
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_IN_COMPLETE_G_CB_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
        #endif
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_IN_COMPLETE_G_CB_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
        #endif
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0);
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P3_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_P3_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
        }
        #endif
      }
      #endif
      #else
      #if L_CB_RES_DEST_LEVEL <= LOCAL && CACHE_L_CB == 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #if (K1_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI > 0
      for (size_t step = 0; step < (K1_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        #endif
        K1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #endif
      #if (K1_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_L_FLAT_NUM_WI > 0
      if (K1_L_FLAT_WI_ID < (K1_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + ((K1_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        #endif
        K1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #endif
      #if L_CB_RES_DEST_LEVEL <= LOCAL
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #endif
      #endif
    
    }
    #endif
    // phase 3: process extra local elements in dimension L_2 with a subset of all FUs
    #if K1_L_NUM_EXTRA_ELEMENTS_L_2 > 0
    if (K1_G_FU_ID_L_2 < (K1_L_NUM_EXTRA_ELEMENTS_L_2 + K1_L_NUM_FU_L_2 - 1) / K1_L_NUM_FU_L_2) {  
      // set step variable for access to res buffer
      const size_t l_step_l_2 = K1_L_NUM_STEPS_L_2 + (K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
      // ---------- L caching --------------------
      #if CACHE_L_CB != 0
      for (size_t step = 0; step < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2)) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / ((2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2));
        const size_t l_index_l_2 = flat_index % (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2);
        K1_P3_L_2_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_P3_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      if (K1_L_FLAT_WI_ID < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2)) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + (((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2)) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / ((2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2));
        const size_t l_index_l_2 = flat_index % (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2);
        K1_P3_L_2_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_P3_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      // ---------- end of L caching -------------
      
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_L_1; ++p_step_l_1) {
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
      
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1;
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
      
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0);
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_L_1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
          #endif
        
        }
      
      }
      #endif
      
      // wait for all WIs to finish computation on local cache block
      #if CACHE_L_CB != 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      
      // move results upwards in memory hierarchy
      #if G_CB_RES_DEST_LEVEL > L_CB_RES_DEST_LEVEL
      
      #if L_CB_RES_DEST_LEVEL < LOCAL
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_L_1; ++p_step_l_1) {
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_PP3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
      
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1;
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_PP3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
      
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0);
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_1_PP3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
        }
      
      }
      #endif
      #else
      #if L_CB_RES_DEST_LEVEL <= LOCAL && CACHE_L_CB == 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      for (size_t step = 0; step < (K1_L_NUM_PROCESSED_ELEMENTS_L_1 * ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        const size_t l_index_l_2 = flat_index % ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2));
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        #endif
        K1_P3_L_2_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_P3_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      if (K1_L_FLAT_WI_ID < (K1_L_NUM_PROCESSED_ELEMENTS_L_1 * ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + ((K1_L_NUM_PROCESSED_ELEMENTS_L_1 * ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        const size_t l_index_l_2 = flat_index % ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2));
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_L_1, ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        #endif
        K1_P3_L_2_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_P3_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #if L_CB_RES_DEST_LEVEL <= LOCAL
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #endif
      #endif
    
    }
    #endif
  } // end of "l_step_l_1"-loop
  #endif
  // phase 2: process extra local iterations in dimension L_1 with all FUs
  #if K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0
  {  
    const size_t l_step_l_1 = K1_L_NUM_STEPS_L_1;
    // phase 1: process local cached iterations in dimension L_2 with all FUs
    #if K1_L_NUM_STEPS_L_2 > 0
    for (size_t l_step_l_2 = 0; l_step_l_2 < K1_L_NUM_STEPS_L_2; ++l_step_l_2) {
      // ---------- L caching --------------------
      #if CACHE_L_CB != 0
      #if ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) / K1_L_FLAT_NUM_WI > 0
      for (size_t step = 0; step < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2;
        K1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      #endif
      #if ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) % K1_L_FLAT_NUM_WI > 0
      if (K1_L_FLAT_WI_ID < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + (((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2;
        K1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      #endif
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      // ---------- end of L caching -------------
      
      
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1; ++p_step_l_1) {
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
        #endif
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
        #endif
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0);
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P3_IN_COMPLETE_G_CB_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_IN_COMPLETE_G_CB_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P3_IN_COMPLETE_G_CB_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_IN_COMPLETE_G_CB_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
          #endif
        
        }
        #endif
      }
      #endif
      
      
      // wait for all WIs to finish computation on local cache block
      #if CACHE_L_CB != 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      
      // move results upwards in memory hierarchy
      #if G_CB_RES_DEST_LEVEL > L_CB_RES_DEST_LEVEL
      
      #if L_CB_RES_DEST_LEVEL < LOCAL
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1; ++p_step_l_1) {
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
        #endif
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
        #endif
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0);
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P3_IN_COMPLETE_G_CB_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_P3_IN_COMPLETE_G_CB_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_IN_COMPLETE_G_CB_L_1_P3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
        }
        #endif
      }
      #endif
      #else
      #if L_CB_RES_DEST_LEVEL <= LOCAL && CACHE_L_CB == 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #if (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_L_FLAT_NUM_WI > 0
      for (size_t step = 0; step < (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_L_NUM_PROCESSED_ELEMENTS_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        #endif
        K1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #endif
      #if (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_L_2) % K1_L_FLAT_NUM_WI > 0
      if (K1_L_FLAT_WI_ID < (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_L_2) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + ((K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_L_NUM_PROCESSED_ELEMENTS_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        #endif
        K1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #endif
      #if L_CB_RES_DEST_LEVEL <= LOCAL
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #endif
      #endif
    
    } // end of "l_step_l_2"-loop
    #endif
    // phase 2: process extra local iterations in dimension L_2 with all FUs
    #if K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
    {  
      const size_t l_step_l_2 = K1_L_NUM_STEPS_L_2;
      // ---------- L caching --------------------
      #if CACHE_L_CB != 0
      #if ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI > 0
      for (size_t step = 0; step < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        K1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      #endif
      #if ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_L_FLAT_NUM_WI > 0
      if (K1_L_FLAT_WI_ID < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + (((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        K1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      #endif
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      // ---------- end of L caching -------------
      
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1; ++p_step_l_1) {
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
        #endif
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1) {
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
        #endif
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0);
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P3_IN_COMPLETE_G_CB_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_IN_COMPLETE_G_CB_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_P3_IN_COMPLETE_G_CB_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_IN_COMPLETE_G_CB_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
          #endif
        
        }
        #endif
      }
      #endif
      
      // wait for all WIs to finish computation on local cache block
      #if CACHE_L_CB != 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      
      // move results upwards in memory hierarchy
      #if G_CB_RES_DEST_LEVEL > L_CB_RES_DEST_LEVEL
      
      #if L_CB_RES_DEST_LEVEL < LOCAL
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1; ++p_step_l_1) {
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_IN_COMPLETE_G_CB_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
        #endif
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_IN_COMPLETE_G_CB_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
        #endif
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0);
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P3_IN_COMPLETE_G_CB_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_P3_IN_COMPLETE_G_CB_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_IN_COMPLETE_G_CB_L_1_P3_IN_COMPLETE_G_CB_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
        }
        #endif
      }
      #endif
      #else
      #if L_CB_RES_DEST_LEVEL <= LOCAL && CACHE_L_CB == 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #if (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI > 0
      for (size_t step = 0; step < (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        #endif
        K1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #endif
      #if (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_L_FLAT_NUM_WI > 0
      if (K1_L_FLAT_WI_ID < (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + ((K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        #endif
        K1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #endif
      #if L_CB_RES_DEST_LEVEL <= LOCAL
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #endif
      #endif
    
    }
    #endif
    // phase 3: process extra local elements in dimension L_2 with a subset of all FUs
    #if K1_L_NUM_EXTRA_ELEMENTS_L_2 > 0
    if (K1_G_FU_ID_L_2 < (K1_L_NUM_EXTRA_ELEMENTS_L_2 + K1_L_NUM_FU_L_2 - 1) / K1_L_NUM_FU_L_2) {  
      // set step variable for access to res buffer
      const size_t l_step_l_2 = K1_L_NUM_STEPS_L_2 + (K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
      // ---------- L caching --------------------
      #if CACHE_L_CB != 0
      for (size_t step = 0; step < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2)) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / ((2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2));
        const size_t l_index_l_2 = flat_index % (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2);
        K1_P3_L_2_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_P3_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      if (K1_L_FLAT_WI_ID < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2)) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + (((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2)) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / ((2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2));
        const size_t l_index_l_2 = flat_index % (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2);
        K1_P3_L_2_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_P3_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      // ---------- end of L caching -------------
      
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1; ++p_step_l_1) {
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
      
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1;
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0(K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1) {
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          #endif
        
        }
      
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0);
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_2_L_MEM_IN(K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
          #endif
        
        }
      
      }
      #endif
      
      // wait for all WIs to finish computation on local cache block
      #if CACHE_L_CB != 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      
      // move results upwards in memory hierarchy
      #if G_CB_RES_DEST_LEVEL > L_CB_RES_DEST_LEVEL
      
      #if L_CB_RES_DEST_LEVEL < LOCAL
      // phase 1: process private cached iterations in dimension L_1 with all FUs
      #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 > 0
      for (size_t p_step_l_1 = 0; p_step_l_1 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1; ++p_step_l_1) {
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_PP3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
      
      } // end of "p_step_l_1"-loop
      #endif
      // phase 2: process extra private iterations in dimension L_1 with all FUs
      #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0
      {  
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1;
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          for (size_t p_iteration_l_1 = 0; p_iteration_l_1 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1; ++p_iteration_l_1)
            {
              size_t p_iteration_l_2 = 0;
              K1_PP3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
        }
      
      }
      #endif
      // phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 > 0
      if (K1_L_FU_ID_L_1 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_1 + K1_P_NUM_FU_L_1 - 1) / K1_P_NUM_FU_L_1) {  
        // set step variable for access to res buffer
        const size_t p_step_l_1 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_1 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_1 > 0);
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_IN_COMPLETE_G_CB_L_1_PP3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
        }
      
      }
      #endif
      #else
      #if L_CB_RES_DEST_LEVEL <= LOCAL && CACHE_L_CB == 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      for (size_t step = 0; step < (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        const size_t l_index_l_2 = flat_index % ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2));
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        #endif
        K1_P3_L_2_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_P3_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      if (K1_L_FLAT_WI_ID < (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + ((K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1 * ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        const size_t l_index_l_2 = flat_index % ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2));
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_1, ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        #endif
        K1_P3_L_2_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_L_TO_G_INDEX_L_1(l_index_l_1), K1_P3_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #if L_CB_RES_DEST_LEVEL <= LOCAL
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #endif
      #endif
    
    }
    #endif
  }
  #endif
  // phase 3: process extra local elements in dimension L_1 with a subset of all FUs
  #if K1_L_NUM_EXTRA_ELEMENTS_L_1 > 0
  if (K1_G_FU_ID_L_1 < (K1_L_NUM_EXTRA_ELEMENTS_L_1 + K1_L_NUM_FU_L_1 - 1) / K1_L_NUM_FU_L_1) {  
    // set step variable for access to res buffer
    const size_t l_step_l_1 = K1_L_NUM_STEPS_L_1 + (K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_1 > 0);
    // phase 1: process local cached iterations in dimension L_2 with all FUs
    #if K1_L_NUM_STEPS_L_2 > 0
    for (size_t l_step_l_2 = 0; l_step_l_2 < K1_L_NUM_STEPS_L_2; ++l_step_l_2) {
      // ---------- L caching --------------------
      #if CACHE_L_CB != 0
      for (size_t step = 0; step < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) + 2)) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2;
        K1_P3_L_1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      if (K1_L_FLAT_WI_ID < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) + 2)) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + (((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) + 2)) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2;
        K1_P3_L_1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      // ---------- end of L caching -------------
      
      
      // parent level in phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      if (K1_L_FU_ID_L_1 < K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) {
        size_t p_step_l_1 = 0;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_PP3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_PP3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_PP3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_PP3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_PP3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_L_CB_L_2;
            #endif
            K1_PP3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_PP3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_PP3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_1_P3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_PP3_L_1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_PP3_L_1_P3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
          #endif
        
        }
        #endif
      }
      
      
      
      // wait for all WIs to finish computation on local cache block
      #if CACHE_L_CB != 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      
      // move results upwards in memory hierarchy
      #if G_CB_RES_DEST_LEVEL > L_CB_RES_DEST_LEVEL
      
      #if L_CB_RES_DEST_LEVEL < LOCAL
      // parent level in phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      if (K1_L_FU_ID_L_1 < K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) {
        size_t p_step_l_1 = 0;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_L_2; ++p_step_l_2) {
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_PP3_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2;
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_L_CB_L_2; ++p_iteration_l_2)
              K1_PP3_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_PP3_L_1_P3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
        }
        #endif
      }
      
      #else
      #if L_CB_RES_DEST_LEVEL <= LOCAL && CACHE_L_CB == 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      for (size_t step = 0; step < (((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) * K1_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), K1_L_NUM_PROCESSED_ELEMENTS_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        #endif
        K1_P3_L_1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      if (K1_L_FLAT_WI_ID < (((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) * K1_L_NUM_PROCESSED_ELEMENTS_L_2) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + ((((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) * K1_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), K1_L_NUM_PROCESSED_ELEMENTS_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), K1_L_NUM_PROCESSED_ELEMENTS_L_2);
        #endif
        K1_P3_L_1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #if L_CB_RES_DEST_LEVEL <= LOCAL
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #endif
      #endif
    
    } // end of "l_step_l_2"-loop
    #endif
    // phase 2: process extra local iterations in dimension L_2 with all FUs
    #if K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0
    {  
      const size_t l_step_l_2 = K1_L_NUM_STEPS_L_2;
      // ---------- L caching --------------------
      #if CACHE_L_CB != 0
      for (size_t step = 0; step < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) + 2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        K1_P3_L_1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      if (K1_L_FLAT_WI_ID < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) + 2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + (((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) + 2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / (K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_IN_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        K1_P3_L_1_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      // ---------- end of L caching -------------
      
      // parent level in phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      if (K1_L_FU_ID_L_1 < K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) {
        size_t p_step_l_1 = 0;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_PP3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2;
            #endif
            K1_PP3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_PP3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_PP3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_PP3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            #else
            const size_t p_index_l_1 = flat_index / (K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
            const size_t p_index_l_2 = flat_index % K1_IN_P_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
            #endif
            K1_PP3_L_1_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2) {
              // process one mda element
              K1_PP3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_PP3_L_1_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
          #endif
        
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_P3_P_TO_L_INDEX_IN_COMPLETE_G_CB_L_2(p_index_l_2));
          }
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
          #endif
        
        }
        #endif
      }
      
      
      // wait for all WIs to finish computation on local cache block
      #if CACHE_L_CB != 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      
      // move results upwards in memory hierarchy
      #if G_CB_RES_DEST_LEVEL > L_CB_RES_DEST_LEVEL
      
      #if L_CB_RES_DEST_LEVEL < LOCAL
      // parent level in phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      if (K1_L_FU_ID_L_1 < K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) {
        size_t p_step_l_1 = 0;
        // phase 1: process private cached iterations in dimension L_2 with all FUs
        #if K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 > 0
        for (size_t p_step_l_2 = 0; p_step_l_2 < K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2; ++p_step_l_2) {
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_L_2; ++p_iteration_l_2)
              K1_PP3_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        } // end of "p_step_l_2"-loop
        #endif
        // phase 2: process extra private iterations in dimension L_2 with all FUs
        #if K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0
        {  
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2;
          {
            size_t p_iteration_l_1 = 0;
            for (size_t p_iteration_l_2 = 0; p_iteration_l_2 < K1_P_NUM_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2; ++p_iteration_l_2)
              K1_PP3_L_1_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
          }
        }
        #endif
        // phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        #if K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 > 0
        if (K1_L_FU_ID_L_2 < (K1_P_NUM_EXTRA_ELEMENTS_IN_COMPLETE_G_CB_L_2 + K1_P_NUM_FU_L_2 - 1) / K1_P_NUM_FU_L_2) {  
          // set step variable for access to res buffer
          const size_t p_step_l_2 = K1_P_NUM_STEPS_IN_COMPLETE_G_CB_L_2 + (K1_P_NUM_EXTRA_CACHED_ITERATIONS_IN_COMPLETE_G_CB_L_2 > 0);
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_PP3_L_1_P3_IN_COMPLETE_G_CB_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
        }
        #endif
      }
      
      #else
      #if L_CB_RES_DEST_LEVEL <= LOCAL && CACHE_L_CB == 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      for (size_t step = 0; step < (((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        #endif
        K1_P3_L_1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      if (K1_L_FLAT_WI_ID < (((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + ((((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) * K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        const size_t l_index_l_2 = flat_index % K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2;
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), K1_L_NUM_PROCESSED_ELEMENTS_IN_COMPLETE_G_CB_L_2);
        #endif
        K1_P3_L_1_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #if L_CB_RES_DEST_LEVEL <= LOCAL
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #endif
      #endif
    
    }
    #endif
    // phase 3: process extra local elements in dimension L_2 with a subset of all FUs
    #if K1_L_NUM_EXTRA_ELEMENTS_L_2 > 0
    if (K1_G_FU_ID_L_2 < (K1_L_NUM_EXTRA_ELEMENTS_L_2 + K1_L_NUM_FU_L_2 - 1) / K1_L_NUM_FU_L_2) {  
      // set step variable for access to res buffer
      const size_t l_step_l_2 = K1_L_NUM_STEPS_L_2 + (K1_L_NUM_EXTRA_CACHED_ITERATIONS_L_2 > 0);
      // ---------- L caching --------------------
      #if CACHE_L_CB != 0
      for (size_t step = 0; step < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) + 2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2)) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / ((2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2));
        const size_t l_index_l_2 = flat_index % (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2);
        K1_P3_L_1_P3_L_2_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_P3_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      if (K1_L_FLAT_WI_ID < ((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) + 2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2)) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + (((K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_1 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) + 2) / K1_IN_L_NUM_PROCESSED_ELEMENTS_L_2 * (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2)) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        const size_t l_index_l_1 = flat_index / ((2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2));
        const size_t l_index_l_2 = flat_index % (2 + ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)) + 2);
        K1_P3_L_1_P3_L_2_L_MEM_IN(l_index_l_1, l_index_l_2) = K1_G_MEM_IN(K1_IN_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_IN_P3_L_TO_G_INDEX_L_2(l_index_l_2));
      }
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      // ---------- end of L caching -------------
      
      // parent level in phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      if (K1_L_FU_ID_L_1 < K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) {
        size_t p_step_l_1 = 0;
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          // ---------- P caching --------------------
          #if CACHE_P_CB != 0
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI > 0
          for (size_t step = 0; step < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI; ++step) {
            const size_t flat_index = K1_P_FLAT_WI_ID + step * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_P3_L_2_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #if ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI > 0
          if (K1_P_FLAT_WI_ID < ((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) % K1_P_FLAT_NUM_WI) {
            const size_t flat_index = K1_P_FLAT_WI_ID + (((K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_1 * (2 + 1 + 2) / K1_IN_P_NUM_PROCESSED_ELEMENTS_L_2 * (2 + 1 + 2)) / K1_P_FLAT_NUM_WI) * K1_P_FLAT_NUM_WI;
            #if CACHE_L_CB != 0
            const size_t BUFFER_IN_INDEX_1(p_index_l_1, p_index_l_2) = flat_index / (BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2)));
            const size_t BUFFER_IN_INDEX_0(p_index_l_1, p_index_l_2) = flat_index % BUFFER_IN_INDEX_0((2 + 1 + 2), (2 + 1 + 2));
            #else
            const size_t p_index_l_1 = flat_index / ((2 + 1 + 2));
            const size_t p_index_l_2 = flat_index % (2 + 1 + 2);
            #endif
            K1_PP3_L_1_PP3_L_2_P_MEM_IN(p_index_l_1, p_index_l_2) = K1_P3_L_1_P3_L_2_L_MEM_IN(K1_IN_PP3_P_TO_L_INDEX_L_1(p_index_l_1), K1_IN_PP3_P_TO_L_INDEX_L_2(p_index_l_2));
          }
          #endif
          #endif
          // ---------- end of P caching -------------
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              // process one mda element
              K1_PP3_L_1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = f(
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + -1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 0, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 1, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -2),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + -1),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 0),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 1),
                  K1_PP3_L_1_PP3_L_2_P_MEM_IN(K1_IN_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1) + 2, K1_IN_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2) + 2)
              );
            
            }
          }
          
          // move results upwards in memory hierarchy
          #if L_CB_RES_DEST_LEVEL > P_CB_RES_DEST_LEVEL
          
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_P3_L_1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_PP3_L_1_PP3_L_2_P_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
          #endif
        
        }
      
      }
      
      
      // wait for all WIs to finish computation on local cache block
      #if CACHE_L_CB != 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #endif
      
      // move results upwards in memory hierarchy
      #if G_CB_RES_DEST_LEVEL > L_CB_RES_DEST_LEVEL
      
      #if L_CB_RES_DEST_LEVEL < LOCAL
      // parent level in phase 3: process extra private elements in dimension L_1 with a subset of all FUs
      if (K1_L_FU_ID_L_1 < K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) {
        size_t p_step_l_1 = 0;
        // parent level in phase 3: process extra private elements in dimension L_2 with a subset of all FUs
        if (K1_L_FU_ID_L_2 < K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) {
          size_t p_step_l_2 = 0;
          {
            size_t p_iteration_l_1 = 0;
            {
              size_t p_iteration_l_2 = 0;
              K1_PP3_L_1_PP3_L_2_P_KERNEL_RES(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2)) = K1_P3_L_1_P3_L_2_L_CB_RES_DEST(K1_P_ITERATION_TO_P_INDEX_L_1(p_iteration_l_1), K1_P_ITERATION_TO_P_INDEX_L_2(p_iteration_l_2));
            }
          }
        }
      
      }
      
      #else
      #if L_CB_RES_DEST_LEVEL <= LOCAL && CACHE_L_CB == 0
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      for (size_t step = 0; step < (((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) * ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))) / K1_L_FLAT_NUM_WI; ++step) {
        const size_t flat_index = K1_L_FLAT_WI_ID + step * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        const size_t l_index_l_2 = flat_index % ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2));
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        #endif
        K1_P3_L_1_P3_L_2_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_P3_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      if (K1_L_FLAT_WI_ID < (((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) * ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))) % K1_L_FLAT_NUM_WI) {
        const size_t flat_index = K1_L_FLAT_WI_ID + ((((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)) * ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))) / K1_L_FLAT_NUM_WI) * K1_L_FLAT_NUM_WI;
        #if K2_P_NUM_FU_L_1 == 1
        const size_t l_index_l_1 = flat_index / (((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        const size_t l_index_l_2 = flat_index % ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2));
        #else
        const size_t DESCENDING_L_DIMS_1(l_index_l_1, l_index_l_2) = flat_index / (DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2))));
        const size_t DESCENDING_L_DIMS_0(l_index_l_1, l_index_l_2) = flat_index % DESCENDING_L_DIMS_0(((((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) <= K1_L_NUM_FU_L_1) * (K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_1 - K1_G_FU_ID_L_1 * K1_L_NUM_FU_L_1) > K1_L_NUM_FU_L_1) * K1_L_NUM_FU_L_1)), ((((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= 0) * 0) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > 0) * ((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) <= K1_L_NUM_FU_L_2) * (K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2)) + (((K1_L_NUM_EXTRA_ELEMENTS_L_2 - K1_G_FU_ID_L_2 * K1_L_NUM_FU_L_2) > K1_L_NUM_FU_L_2) * K1_L_NUM_FU_L_2)));
        #endif
        K1_P3_L_1_P3_L_2_L_KERNEL_RES(l_index_l_1, l_index_l_2) =
        #if L_CB_RES_DEST_LEVEL < LOCAL
          K1_L_REDUCTION_MEM(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == LOCAL
          K1_RES_L_BUFFER(l_index_l_1, l_index_l_2);
        #elif L_CB_RES_DEST_LEVEL == GLOBAL
          K1_RES_G_BUFFER(K1_P3_L_TO_G_INDEX_L_1(l_index_l_1), K1_P3_L_TO_G_INDEX_L_2(l_index_l_2));
        #endif
      }
      #if L_CB_RES_DEST_LEVEL <= LOCAL
      barrier(CLK_LOCAL_MEM_FENCE);
      #elif L_CB_RES_DEST_LEVEL == GLOBAL
      barrier(CLK_GLOBAL_MEM_FENCE);
      #endif
      #endif
      #endif
    
    }
    #endif
  }
  #endif
}
// =============== end of kernel 1 ============================================
